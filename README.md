# project_testAPI

Small API test automation project using CSV-driven test data, pytest, and simple HTML/DOCX reporting.

## What this project does

- Reads test case metadata from `input_files/test_case.csv` and test data (payloads and expected status codes) from `input_files/test_data.csv`.
- Runs parameterized API tests (POST then GET) in `tests/test_api.py` using `pytest` and `requests`.
- Collects per-test results into a session context and, on test session teardown, generates:
  - an HTML report: `test_reports/test_report.html`
  - per-test DOCX files in `test_results/` (appended with request/response details for passed cases)
- Optionally exports base DOCX test case files from the Word template via `test_result_generator.py`.

## Repository layout (key files)

- `config/config.py` â€” paths and constants used across the project.
- `input_files/test_case.csv` â€” test case metadata (tc_no, tc_name, tc_description).
- `input_files/test_data.csv` â€” test data rows (tc_no,userId,title,body,post_status_code,get_status_code,id).
- `tests/test_api.py` â€” main pytest test and the session fixture that aggregates results and triggers report generation.
- `tests/helper_functions/write_to_csv.py` â€” helper that updates `input_files/test_data.csv` with returned IDs.
- `utils/report_generator/` â€” HTML report template, CSS/JS, and `generate_report.py` that writes and opens the report.
- `utils/docx_generator/` â€” Word template and `export_to_docx.py` used to append request/response details.
- `test_result_generator.py` â€” generates base DOCX files from `input_files/test_case.csv` and the Word template.

## Quick start (Windows PowerShell)

1. Install the minimal Python requirements (recommended):

```powershell
python -m pip install pytest requests python-docx
```

2. (Optional) Generate base DOCX files for each test case (so `export_all_results_to_docx` can append to them):

```powershell
python .\test_result_generator.py
```

3. Run the tests (this will run all parametrized cases, produce `test_reports/test_report.html`, and attempt to append to DOCX files for passed tests):

```powershell
pytest -q
```

4. If the report does not open automatically, open it manually:

```powershell
ii .\test_reports\test_report.html
```

## Test flow (concise)

1. `pytest` parametrizes tests using a merged view of `test_case.csv` and `test_data.csv`.
2. For each test row, `test_post_and_get` does:
   - POST to `BASE_URL` with JSON payload.
   - Capture POST response and an assertion on `post_status_code`.
   - Extract returned ID and write it back to `input_files/test_data.csv` (helper `update_csv_with_id`).
   - GET the resource using the returned identifier and assert on `get_status_code`.
   - Store request/response/assertion results in the shared session context.
3. After all tests finish, the session fixture generates the HTML report and appends request/response details into DOCX files for passed cases.

## Data format (CSV)

- `input_files/test_case.csv`: columns `tc_no,tc_name,tc_description`.
- `input_files/test_data.csv`: columns `tc_no,userId,title,body,post_status_code,get_status_code,id`.

The test runner merges rows by `tc_no` to build each parametrized test case.

## Outputs

- HTML report: `test_reports/test_report.html` (opens in browser on generation).
- DOCX per-test files: `test_results/` (base DOCX generated by `test_result_generator.py`, then appended for passed cases by `utils.docx_generator.export_to_docx`).

## Known issue & quick fix

- The current test code uses `userId` from the POST response when performing the GET. Typically the created resource `id` should be used for subsequent GETs and for writing back to CSV. This can cause incorrect GETs or flaky assertions.

Quick fix (in `tests/test_api.py`):
- Change the GET id source to use the POST response `id` (e.g., `post_id = post_result['response_body'].get('id')`).
- Ensure `update_csv_with_id` is given the same `id` value when writing back the CSV.

## Recommendations

- Add a `requirements.txt` with pinned versions for reproducibility.
- Add timeouts and exception handling for network calls (use `requests` timeout and catch `requests.exceptions.RequestException`).
- Avoid in-place modification of `input_files/test_data.csv` during test runs; write returned IDs to a separate results file to avoid race conditions.
- Consider a mock or stub server for deterministic CI runs instead of depending on external APIs.

## Next steps I can do for you

- Fix the POST->GET id bug and run tests locally to confirm.
- Add `requirements.txt`.
- Add a short unit test for `update_csv_with_id`.

If you want any of the above changes applied, tell me which one to implement next.





https://chatgpt.com/s/t_68adc91055408191aeb04d19c5a818e2
https://chatgpt.com/s/t_68adcba9d59881919dfc946766821de2

## 1) High-level overview
This repo is a small API test automation project that:
- Uses CSV files for test metadata (test_case.csv) and test data (test_data.csv).
- Runs parameterized tests with pytest (test_api.py) to perform a POST then a GET against an API (currently `https://jsonplaceholder.typicode.com/posts`).
- Collects per-test results into an in-memory session context fixture.
- On session teardown it generates an HTML report (test_report.html) and exports DOCX test-case files (stored in test_results) with appended request/response details for passed test cases.
- Has a helper to write returned IDs back into test_data.csv.
- Has a small utility (test_result_generator.py) that generates base .docx test-case files from test_case.csv and a Word template.

Primary files (short purpose)
- config.py â€” paths and constants (TEST_DATA_PATH, TEST_CASE_PATH, TEST_RESULTS_DIR, TEST_REPORTS_DIR, DOCX_TEMPLATE_PATH).
- test_case.csv â€” test case metadata (tc_no, tc_name, tc_description).
- test_data.csv â€” test data rows (tc_no,userId,title,body,post_status_code,get_status_code,id).
- test_api.py â€” main pytest test, CSV readers, POST/GET helpers, session fixture that aggregates results, teardown triggers report + DOCX export.
- write_to_csv.py â€” writes returned ID back to test_data.csv.
- generate_report.py â€” builds HTML report by inlining CSS/JS into template and opening it in browser.
- export_to_docx.py â€” appends request/response details to an existing DOCX file for passed cases.
- test_result_generator.py â€” creates the base DOCX per test case from a template template.docx.
- test_reports and test_results â€” output folders for HTML and DOCX artifacts.

## 2) Data shapes / contracts
- test_case.csv rows: { "tc_no", "tc_name", "tc_description" }.
- test_data.csv rows: { "tc_no", "userId", "title", "body", "post_status_code", "get_status_code", "id" }.
- Combined per-parametrize `row` (in test_api.py) is a merged dict: fields from test_case.csv (keyed by `tc_no`) plus the fields from test_data.csv.
- Result dict appended to context for each test:
  {
    "tc_no", "scenario" (tc_name), "description",
    "status" ("Passed"/"Failed"/"Error"),
    "requests": [ per-request dict ],
    optional "error_message","error_type","error_traceback","error_time"
  }
- Per-request dict (what report shows): result_heading, method, url, headers, body, status_code, response_headers, response_body, assertions (list of {assertion, expected, actual, result})

Contract expectations
- POST expected status code = `post_status_code` column (e.g., 201).
- GET expected status code = `get_status_code` (e.g., 200).
- `update_csv_with_id(tc_no, post_id)` expects to find row matching `tc_no` to set `id` column.

## 3) Full test execution flow (step-by-step)
1. Pytest start -> `@pytest.fixture(scope="session") test_context()` creates `{"all_results": []}` and yields it.
2. `get_combined_test_data()` reads `TEST_CASE_PATH` and `TEST_DATA_PATH` and produces a list of merged rows. `@pytest.mark.parametrize("row", get_combined_test_data())` runs tests for each row.
3. For each `row` test (function `test_post_and_get`):
   - Initialize `result` object with metadata and empty `requests`.
   - run POST: `run_post_request(row, int(row['post_status_code']))`
     - Builds payload: {"userId": int(row["userId"]), "title": row["title"], "body": row["body"]}
     - Calls `requests.post(BASE_URL, json=payload)`
     - Collects request/response info into a dict with an assertion: status_code == expected
   - Append POST result to `result["requests"]`.
   - Extracts two values:
     - `post_id = post_result["response_body"].get("userId")`  (used for GET)
     - `post_id_csv = post_result["response_body"].get("id")`   (written to CSV)
   - If `post_id` truthy, call `update_csv_with_id(row["tc_no"], post_id_csv)` to write `id` into test_data.csv.
   - run GET: `run_get_request(post_id, int(row['get_status_code']))` -> `requests.get(BASE_URL/{post_id})`, collects response dict and an assertion on status_code.
   - Append GET result to `result["requests"]`.
   - Evaluate all assertions for all requests; if any false set `result["status"] = "Failed"`.
   - On exceptions, set status "Error" and capture traceback/time/etc.
   - Finally append `result` to `test_context["all_results"]`.
4. After session (fixture teardown):
   - `generate_html_report(test_results=context["all_results"], ...)` creates HTML, inlines style.css and `static/script.js` into report_template.html, writes to `TEST_REPORT_PATH` and opens it in browser.
   - `export_all_results_to_docx(test_results=context["all_results"], output_dir=TEST_RESULTS_DIR)`:
     - For each result with `"status" == "Passed"`, it searches `output_dir` for an existing DOCX that begins with `tc_no` (generated earlier by test_result_generator.py). If found, `export_response_to_docx` appends headings, request/response bodies and headers to that DOCX.
     - If base docx missing, it warns and skips.

## 4) Report generation details
- HTML report:
  - Summary metrics: total/passed/failed/error/pass%.
  - An accordion per test showing request/response/assertions and tracebacks (if error).
  - JSON views are pretty-printed and wrapped in a `.json-view` container with copy buttons.
  - The report file is written to `TEST_REPORT_PATH` (from config.py), then opened via `webbrowser.open`.
- DOCX results:
  - test_result_generator.py reads test_case.csv and for each case writes a base DOCX file using template.docx. Filename safe-guarding and path length truncation present.
  - `export_all_results_to_docx` appends only for passed tests. It will fail silently with a warning if the base docx is not present or if permission denied when saving.

## 5) CSV write-back behavior
- `update_csv_with_id(tc_no, post_id)`:
  - Loads test_data.csv as a DictReader, keeps `fieldnames`.
  - Adds `"id"` to `fieldnames` if missing.
  - Finds the row where `row["tc_no"] == tc_no` and sets `row["id"] = str(post_id)`.
  - Writes the file back (overwrites original CSV) with writer.writeheader() and writer.writerows(rows).
- The CSV file path is built relative to the helper file with `os.path.join(..., "..", "..", "input_files", "test_data.csv")`.



## Project Structure:
<pre>
<a href="./">ğŸ“ project_testAPI</a>
â”œâ”€â”€ <a href="./.gitignore">ğŸ“„ .gitignore</a>
â”œâ”€â”€ <a href="./README.md">ğŸ“„ README.md</a>
â”œâ”€â”€ <a href="./config/">ğŸ“ config</a>
â”‚   â””â”€â”€ <a href="./config/config.py">ğŸ“„ config.py</a>
â”œâ”€â”€ <a href="./flow.txt">ğŸ“„ flow.txt</a>
â”œâ”€â”€ <a href="./input_files/">ğŸ“ input_files</a>
â”‚   â”œâ”€â”€ <a href="./input_files/test_case.csv">ğŸ“„ test_case.csv</a>
â”‚   â””â”€â”€ <a href="./input_files/test_data.csv">ğŸ“„ test_data.csv</a>
â”œâ”€â”€ <a href="./openapi_specifications.json">ğŸ“„ openapi_specifications.json</a>
â”œâ”€â”€ <a href="./project_structure.txt">ğŸ“„ project_structure.txt</a>
â”œâ”€â”€ <a href="./project_structure_generator.py">ğŸ“„ project_structure_generator.py</a>
â”œâ”€â”€ <a href="./test_result_generator.py">ğŸ“„ test_result_generator.py</a>
â”œâ”€â”€ <a href="./tests/">ğŸ“ tests</a>
â”‚   â”œâ”€â”€ <a href="./tests/helper_functions/">ğŸ“ helper_functions</a>
â”‚   â”‚   â”œâ”€â”€ <a href="./tests/helper_functions/csv_reader.py">ğŸ“„ csv_reader.py</a>
â”‚   â”‚   â””â”€â”€ <a href="./tests/helper_functions/write_to_csv.py">ğŸ“„ write_to_csv.py</a>
â”‚   â””â”€â”€ <a href="./tests/test_api.py">ğŸ“„ test_api.py</a>
â””â”€â”€ <a href="./utils/">ğŸ“ utils</a>
    â”œâ”€â”€ <a href="./utils/docx_generator/">ğŸ“ docx_generator</a>
    â”‚   â”œâ”€â”€ <a href="./utils/docx_generator/export_to_docx.py">ğŸ“„ export_to_docx.py</a>
    â”‚   â””â”€â”€ <a href="./utils/docx_generator/template.docx">ğŸ“„ template.docx</a>
    â””â”€â”€ <a href="./utils/report_generator/">ğŸ“ report_generator</a>
        â”œâ”€â”€ <a href="./utils/report_generator/generate_report.py">ğŸ“„ generate_report.py</a>
        â”œâ”€â”€ <a href="./utils/report_generator/static/">ğŸ“ static</a>
        â”‚   â”œâ”€â”€ <a href="./utils/report_generator/static/script.js">ğŸ“„ script.js</a>
        â”‚   â””â”€â”€ <a href="./utils/report_generator/static/style.css">ğŸ“„ style.css</a>
        â””â”€â”€ <a href="./utils/report_generator/templates/">ğŸ“ templates</a>
            â””â”€â”€ <a href="./utils/report_generator/templates/report_template.html">ğŸ“„ report_template.html</a>
</pre>